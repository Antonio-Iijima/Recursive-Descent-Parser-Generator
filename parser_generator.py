from utils import *



def generate(syntax: list[str], semantics, debug: bool = False) -> set:
    """Generates a recursive descent parser from an EBNF grammar (BNF with `|` for convenience) for context-free languages."""

    print("Compiling grammar...")

    print()

    rules = {
        rule.strip() : [alternative.strip() for alternative in pattern.split("|")] for rule, pattern in (line.split("::=") for line in syntax)
    }
    
    offset = max(len(p) for p in rules)

    for p, r in rules.items(): 
        print(f"{p}{" " * (offset - len(p))} ::= {" | ".join("".join(s) for s in r)}")
    
    print()

    terminals = set(
        token
                    for alternatives in rules.values()
                for pattern in alternatives
            for token in tokenize(pattern)
        if is_terminal(token)
        )

    operators = list(
        embed_nonterminal(rule) for rule, alternatives in rules.items()
            if any(t in tokenize(alternative) for t in terminals for alternative in alternatives)
        )

    parser_text = f"""'''!!! THIS FILE IS AUTOMATICALLY GENERATED - PLEASE DO NOT MODIFY !!!'''      



from utils import *{"""
from rich import traceback
traceback.install(show_locals=True)
""" if debug else ""}



##### MORE UTILS #####



def parse(string: str) -> 'Rule':
    return {embed_nonterminal(list(rules.keys())[0])}(string)

    
def validate(expr: str) -> str:
    parsed = parse(expr)
    return f"{{expr}} is not a valid expression." if (parsed == None) else f"{{parsed}} is a valid {{parsed.name}} expression."


def is_terminal(token: str) -> bool: return token in TERMINALS


def split(expr: str, sep: list, reversed: bool = False) -> list|None:{"""
    print(f"split: {expr} with {sep}")""" if debug else ""}
    
    if not sep: return [expr] if expr else []

    x = sep.pop()
    if x in expr:
        if x == "":
            if len(expr) > 1:
                left, right = split(expr[:-1], sep, reversed), expr[-1]
            else:
                return [expr]
        else:
            left, right = expr.rsplit(x, 1)
            left = split(left, sep, reversed)
        
        return None if (left == None) else left + [x] + ([right] if right else [])

    return None

    

##### PARSER #####



class Rule:
    def __hash__(self):
        return self.name.__hash__()

        
    def __repr__(self):
        return f"{{self.name[1:-1]}}[{{str(self)}}]"
            
                
    def __str__(self):
        if len(self.children) == 1 and self.children[0] in TERMINALS: return str(self.children[0])
        else:
            s = ""
            i = 0
            for token in self.pattern:
                if token in TERMINALS:
                    s += token
                else:
                    s += str(self.children[i])
                    i += 1
        return s
"""
    
    history = []
    for (rule, alternatives) in rules.items():
        history.append(rule)

        alternatives = [tokenize(pattern) for pattern in alternatives]

        docstring = f"{rule} ::= {" | ".join("".join(pattern) for pattern in alternatives)}"
        logic = f"""
class {embed_nonterminal(rule)}(Rule):
    '''
    Rule: `{docstring}`
    '''

    name: str = "{rule}"

    def __new__(rule, expr: str) -> Rule|None:
"""
        
        nonterminalShortcut = []
        terminalShortcut = []
          
        for idx, pattern in enumerate(alternatives):
            leftRecursive = pattern[0] in history

            if len(pattern) == 1:
                if is_nonterminal(pattern[0]): nonterminalShortcut.append(f"{embed_nonterminal(pattern[0])}(expr)")
                elif is_terminal(pattern[0]): terminalShortcut.append(f"'{pattern[0]}'")

            else:                
                print(f"{"Left" if leftRecursive else "Right"} Recursion for {"".join(pattern)}")
 
                logic += f"""
        # Begin Pattern {idx}: {"".join(pattern)}
        # {"Left" if leftRecursive else "Right"} Recursive
        tokens = []
        pattern = split(expr, {[t for t in pattern if t in terminals]}, {leftRecursive})
        {'print(f"split expr in {rule.name}:", pattern)' if debug else ""}
        # Validate Pattern {idx}
        if not (pattern == None) and len(pattern) == {len(pattern)}:
            for token, test in zip(pattern, [{", ".join(embed_nonterminal(t) if is_nonterminal(t) else f"'{t}'" for t in pattern)}]):            
                valid = (token == test or None) if isinstance(test, str) else test(token)
                if not valid: return {"print(f'FAILED: Pattern mismatch in {rule}')" if debug else "None"}
                tokens.append(valid)

            # Complete Pattern {idx}
            {"""print("tokens", list(t for t in tokens if isinstance(t, Rule)))
            print("pattern", pattern)
            """ if debug else ""}
            instance = object.__new__(rule)
            instance.children = [t for t in tokens if isinstance(t, Rule)]
            instance.pattern = pattern
            {"print(f'PASSED {rule}: Continuing')" if debug else ""}
            # print(f"{{instance.__repr__()}}")
            return instance
"""
                
        logic += (f"""
        # Terminal shortcut
        if expr in ({",".join(terminalShortcut)}):
            instance = object.__new__(rule)
            instance.children = [expr]
            # print(f"{{instance.__repr__()}} => ", end='')
            return instance
"""  if terminalShortcut else "") + f"""
        {"# Nonterminal shortcut" if nonterminalShortcut else ""}
        return {" or ".join(nonterminalShortcut) if nonterminalShortcut else ("print(f'FAILED {rule}[ {expr} ]: Backtracking')" if debug else "None")}
"""
        
        parser_text += f"""
""" + logic

    parser_text += f"""
    
RULES = {{
    {",\n    ".join(f"{embed_nonterminal(rule)}{" " * (offset - len(rule))} : {alternatives}" 
                    for rule, alternatives in rules.items())}
    }}
    
TERMINALS = {terminals}
"""


    with open("parser.py", "w") as file:
        file.write(parser_text)

    print()
    with open("eval.py", "w") as file:
        print("Generating eval...")
        file.write(generate_eval(operators, semantics))
        
    print("\nDone!")
 


def generate_eval(operators: set[str], semantics: str) -> str:
    # AST-bound operations should be prefixed with 'p_'; 
    # global functions and variables (e.g. the environment) with 'g_'

    eval_text = f"""'''!!! THIS FILE IS AUTOMATICALLY GENERATED - PLEASE DO NOT MODIFY !!!'''      
    
    

from importlib.machinery import SourceFileLoader
s = SourceFileLoader("semantics", "{semantics}").load_module()



##### EVAL #####


def evaluate(AST):
    match type(AST).__name__:
"""
        
    for operation in operators:
        eval_text += f"""
        case "{operation}": 
            return s.p_{operation.lower()}(*map(evaluate, AST.children))
"""

    eval_text += """
        case "NoneType":
            return ""

        case _: 
            return AST
"""

    return eval_text
