from utils import *



def process_syntax(syntax: list[str]) -> dict:
    return {
        rule.strip() : [[s for s in split_nonterminals(alternative.strip()) if not s == ""] for alternative in pattern.split("|")]
        for rule, pattern in (line.split("::=") for line in syntax)
    }


def process_semantics(semantics: str) -> str:
    """Current implementation does nothing."""
    return semantics


def show_grammar(grammar: dict) -> None:
    offset = max(len(p) for p in grammar)

    for p, r in grammar.items(): 
        print(f"{p}{" " * (offset - len(p))} ::= {" | ".join("".join(s) for s in r)}")
    
    print()

    return offset


def generate_AST(syntax: list[str], semantics, debug: bool = False) -> set:
    """Generates an AST from an EBNF grammar (BNF with `|` for convenience) for context-free languages."""

    print("Compiling grammar...")

    print()

    GRAMMAR = process_syntax(syntax)
    offset = show_grammar(GRAMMAR)

    TERMINALS = set(
        token
                for alternatives in GRAMMAR.values()
            for pattern in alternatives
        for token in pattern if is_terminal(token)
        )

    AST_text = f"""'''!!! THIS FILE IS AUTOMATICALLY GENERATED - PLEASE DO NOT MODIFY !!!'''      



from parser import Rule



##### ABSTRACT SYNTAX TREE #####


"""
    
    for (rule, alternatives) in GRAMMAR.items():

        docstring = f"{rule} ::= {" | ".join("".join(pattern) for pattern in alternatives)}"
        AST_text += f"""
class {embed_nonterminal(rule)}(Rule):
    '''Rule: `{docstring}`'''
    name: str = "{rule}"
"""

    AST_text += f"""

        
##### GRAMMAR #####


    
GRAMMAR = {{
    {",\n    ".join(f'''{embed_nonterminal(rule)}{" " * (offset - len(rule))} : [{
        ",".join(f"[{", ".join(embed_nonterminal(s) if is_nonterminal(s) else f"'{s}'" for s in alternative)}]"
                for alternative in alternatives)}]'''
                    for rule, alternatives in GRAMMAR.items())}
}}


TERMINALS = {TERMINALS}


TOKENS = TERMINALS.union(GRAMMAR.keys())


# Pre-compute expected subsequent tokens for any given token for arbitrary lookahead
EXPECTED_TOKENS = {{ token : [] for token in TOKENS }}

for token in TOKENS:
    for rule in GRAMMAR.values():
        for alternative in rule:
            
            if token in alternative:
                for i, t in enumerate(alternative[:-1]):

                    if t == token:
                        EXPECTED_TOKENS[token].append(alternative[i+1])


def is_expected(e, x: Rule|str) -> list:
    def retype(x): return type(x) if isinstance(x, Rule) else x
    return retype(e) in EXPECTED_TOKENS[retype(x)]
"""


    with open("AST.py", "w") as file:
        file.write(AST_text)

    print()
    with open("eval.py", "w") as file:
        print("Generating eval...")
        file.write(generate_eval(GRAMMAR, semantics))
        
    print("\nDone!")
 


def generate_eval(grammar: dict, semantics: str) -> str:
    # AST-bound operations should be prefixed with 'p_'; 
    # global functions and variables (e.g. the environment) with 'g_'

    eval_text = f"""'''!!! THIS FILE IS AUTOMATICALLY GENERATED - PLEASE DO NOT MODIFY !!!'''      
    
    

from importlib.machinery import SourceFileLoader
s = SourceFileLoader("semantics", "{semantics}").load_module()



##### EVAL #####



def evaluate(AST):
    from AST import Rule

    if isinstance(AST, Rule):
        expr = list(map(evaluate, AST.children))

        match type(AST).__name__:
"""
        
    for rule in grammar.keys():

        eval_text += f"""
            case "{embed_nonterminal(rule)}": 
                return s.p_{embed_nonterminal(rule).lower()}(expr)
"""

    eval_text += """
    return "" if AST == None else AST
"""

    return eval_text
